{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab3f71d-9c49-4a1d-b84a-e366ccf38ee9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 339 bytes.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/log_data/sample_logs.csv</td><td>sample_logs.csv</td><td>339</td><td>1717026888000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/log_data/sample_logs.csv",
         "sample_logs.csv",
         339,
         1717026888000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "dbutils.fs.mkdirs(\"/FileStore/log_data/\")\n",
    "\n",
    "# Upload the file content\n",
    "dbutils.fs.put(\"/FileStore/log_data/sample_logs.csv\", \"\"\"\n",
    "ip,timestamp,endpoint,response_code,content_size\n",
    "192.168.1.1,2023-05-01T00:00:01Z,/api/v1/resource,200,512\n",
    "192.168.1.2,2023-05-01T00:00:02Z,/api/v1/resource,404,0\n",
    "192.168.1.1,2023-05-01T00:00:03Z,/api/v1/resource,200,256\n",
    "192.168.1.3,2023-05-01T00:00:04Z,/api/v1/resource,500,128\n",
    "192.168.1.1,2023-05-01T00:00:05Z,/api/v1/resource,200,1024\n",
    "\"\"\", True)\n",
    "\n",
    "# List files in the /FileStore/log_data directory to verify upload\n",
    "display(dbutils.fs.ls(\"dbfs:/FileStore/log_data/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae876d29-f847-4e9c-b47f-44f0a58aa994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------------+-------------+------------+\n|         ip|          timestamp|        endpoint|response_code|content_size|\n+-----------+-------------------+----------------+-------------+------------+\n|192.168.1.1|2023-05-01 00:00:01|/api/v1/resource|          200|         512|\n|192.168.1.2|2023-05-01 00:00:02|/api/v1/resource|          404|           0|\n|192.168.1.1|2023-05-01 00:00:03|/api/v1/resource|          200|         256|\n|192.168.1.3|2023-05-01 00:00:04|/api/v1/resource|          500|         128|\n|192.168.1.1|2023-05-01 00:00:05|/api/v1/resource|          200|        1024|\n+-----------+-------------------+----------------+-------------+------------+\n\n+-----------+-------------------+----------------+-------------+------------+\n|         ip|          timestamp|        endpoint|response_code|content_size|\n+-----------+-------------------+----------------+-------------+------------+\n|192.168.1.1|2023-05-01 00:00:01|/api/v1/resource|          200|         512|\n|192.168.1.2|2023-05-01 00:00:02|/api/v1/resource|          404|           0|\n|192.168.1.1|2023-05-01 00:00:03|/api/v1/resource|          200|         256|\n|192.168.1.3|2023-05-01 00:00:04|/api/v1/resource|          500|         128|\n|192.168.1.1|2023-05-01 00:00:05|/api/v1/resource|          200|        1024|\n+-----------+-------------------+----------------+-------------+------------+\n\nroot\n |-- ip: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- endpoint: string (nullable = true)\n |-- response_code: integer (nullable = true)\n |-- content_size: integer (nullable = true)\n\n+-------+-----------+----------------+------------------+-----------------+\n|summary|         ip|        endpoint|     response_code|     content_size|\n+-------+-----------+----------------+------------------+-----------------+\n|  count|          5|               5|                 5|                5|\n|   mean|       null|            null|             300.8|            384.0|\n| stddev|       null|            null|142.13796115042595|404.7715405015526|\n|    min|192.168.1.1|/api/v1/resource|               200|                0|\n|    max|192.168.1.3|/api/v1/resource|               500|             1024|\n+-------+-----------+----------------+------------------+-----------------+\n\nTotal number of log entries: 5\n+-------------+-----+\n|response_code|count|\n+-------------+-----+\n|          200|    3|\n|          500|    1|\n|          404|    1|\n+-------------+-----+\n\n+-----------+-----+\n|         ip|count|\n+-----------+-----+\n|192.168.1.1|    3|\n|192.168.1.2|    1|\n|192.168.1.3|    1|\n+-----------+-----+\n\n+--------+--------+--------+\n|min_size|max_size|avg_size|\n+--------+--------+--------+\n|       0|    1024|   384.0|\n+--------+--------+--------+\n\n+----------------+----------+\n|        endpoint|total_size|\n+----------------+----------+\n|/api/v1/resource|      1920|\n+----------------+----------+\n\n+-------------+-----+\n|response_code|count|\n+-------------+-----+\n|          200|    3|\n|          500|    1|\n|          404|    1|\n+-------------+-----+\n\n+---+-----+\n| ip|count|\n+---+-----+\n+---+-----+\n\n+-------------------+----------------+\n|          timestamp|        endpoint|\n+-------------------+----------------+\n|2023-05-01 00:00:02|/api/v1/resource|\n+-------------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, min, max, avg, sum, desc\n",
    "\n",
    "# Load the log data from the uploaded CSV file\n",
    "log_df = spark.read.csv(\"dbfs:/FileStore/log_data/sample_logs.csv\", header=True, inferSchema=True)\n",
    "log_df.show(5)\n",
    "\n",
    "# Clean and filter the logs\n",
    "filtered_logs = log_df.select(\"ip\", \"timestamp\", \"endpoint\", \"response_code\", \"content_size\") \\\n",
    "    .filter(col(\"response_code\").isNotNull())\n",
    "filtered_logs.show(5)\n",
    "\n",
    "# Basic Data Exploration\n",
    "filtered_logs.printSchema()\n",
    "filtered_logs.describe().show()\n",
    "log_count = filtered_logs.count()\n",
    "print(f\"Total number of log entries: {log_count}\")\n",
    "\n",
    "# Analyze Trends Based on Response Codes and Traffic\n",
    "response_code_counts = filtered_logs.groupBy(\"response_code\").agg(count(\"response_code\").alias(\"count\")).orderBy(\"count\", ascending=False)\n",
    "response_code_counts.show()\n",
    "\n",
    "# Top 10 most frequent IP addresses\n",
    "top_ips = filtered_logs.groupBy(\"ip\").agg(count(\"ip\").alias(\"count\")).orderBy(\"count\", ascending=False).limit(10)\n",
    "top_ips.show()\n",
    "\n",
    "# Calculate Content Size Statistics\n",
    "content_size_stats = filtered_logs.select(\n",
    "    min(\"content_size\").alias(\"min_size\"),\n",
    "    max(\"content_size\").alias(\"max_size\"),\n",
    "    avg(\"content_size\").alias(\"avg_size\")\n",
    ")\n",
    "content_size_stats.show()\n",
    "\n",
    "# Top Endpoints by Content Size\n",
    "top_endpoints_by_size = filtered_logs.groupBy(\"endpoint\").agg(sum(\"content_size\").alias(\"total_size\")).orderBy(\"total_size\", ascending=False).limit(10)\n",
    "top_endpoints_by_size.show()\n",
    "\n",
    "# Analyze Response Codes\n",
    "response_code_stats = filtered_logs.groupBy(\"response_code\").count().orderBy(\"count\", ascending=False)\n",
    "response_code_stats.show()\n",
    "\n",
    "# Identify IP Addresses Accessing Server More Than 10 Times\n",
    "frequent_ips = filtered_logs.groupBy(\"ip\").count().filter(col(\"count\") > 10).orderBy(\"count\", ascending=False)\n",
    "frequent_ips.show()\n",
    "\n",
    "# Analyze Bad Requests (404 Errors)\n",
    "bad_requests = filtered_logs.filter(col(\"response_code\") == 404)\n",
    "latest_404_requests = bad_requests.orderBy(col(\"timestamp\").desc()).select(\"timestamp\", \"endpoint\").limit(10)\n",
    "latest_404_requests.show()\n",
    "\n",
    "# Save Results to CSV Files\n",
    "response_code_stats.write.csv(\"dbfs:/FileStore/log_data/response_code_stats.csv\", header=True)\n",
    "frequent_ips.write.csv(\"dbfs:/FileStore/log_data/frequent_ips.csv\", header=True)\n",
    "latest_404_requests.write.csv(\"dbfs:/FileStore/log_data/latest_404_requests.csv\", header=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Anomalies Detection 2024-05-29 18:49:36",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
